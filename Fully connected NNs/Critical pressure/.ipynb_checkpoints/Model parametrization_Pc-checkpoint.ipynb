{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa5289a-3bba-4ce5-ba4a-fc7c5aa0b8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 12:45:40,926\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-02-21 12:45:41,180\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "import pickle\n",
    "from ray import train\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import os\n",
    "from functools import partial\n",
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02068672-5ba2-46b7-9e3e-2518d80af039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abs_path is made for function \"load_data\" so that is doesn't lose table with descriptors\n",
    "abs_path = os.path.abspath('.')+'\\\\table_with_desriptors.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f416b3b-9b4c-4f69-8838-6550973ec11c",
   "metadata": {},
   "source": [
    "The function \"load_data\" according to its name, loads data from descriptors table. Actually, it unites loading from .csv file, dropping all above 95% percentile, scaling of numeric descriptors and creating train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d488d9-ee5f-46f6-a27d-436b57da05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(abs_path = abs_path):\n",
    "    #reading .csv file\n",
    "    descriptors = pd.read_csv(abs_path, index_col = 0)\n",
    "    descriptors = descriptors[~descriptors['Pc'].isnull()]\n",
    "    #getting X and Y\n",
    "    y = descriptors['Pc']\n",
    "    descriptors = descriptors.drop(columns = ['SMILES', 'Tc', 'Pc', 'omega', 'mol'])\n",
    "    #dropping all above 95% percentile\n",
    "    quant = y.quantile(q = 0.95)\n",
    "    mask = y < quant\n",
    "    y = y[mask]\n",
    "    descriptors = descriptors[mask]\n",
    "   #splitting the dataset into train test and valid\n",
    "    X_train, X_test, y_train, y_test = train_test_split(descriptors, y, test_size = 0.15, random_state=0)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.15, random_state = 0)\n",
    "    #scaling columns\n",
    "    columns_to_scale = list(X_train.columns[:156])\n",
    "    ct = ColumnTransformer([('Scaler', MinMaxScaler(),columns_to_scale)], remainder= 'passthrough')\n",
    "    X_train = ct.fit_transform(X_train)\n",
    "    X_test = ct.transform(X_test)\n",
    "    X_valid = ct.transform(X_valid)\n",
    "    #gettin datasets\n",
    "    X_train_ds = TensorDataset(torch.tensor(X_train, dtype = torch.float32), torch.tensor(y_train.values, dtype = torch.float32))\n",
    "    X_test_ds = TensorDataset(torch.tensor(X_test, dtype = torch.float32), torch.tensor(y_test.values, dtype = torch.float32))\n",
    "    X_valid_ds = TensorDataset(torch.tensor(X_valid, dtype = torch.float32), torch.tensor(y_valid.values, dtype = torch.float32))\n",
    "    return X_train_ds, X_valid_ds, X_test_ds, ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c2816-b070-4c8f-a0d8-dd147e9dcb90",
   "metadata": {},
   "source": [
    "In the cell below we check that function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5081a23f-39f9-44d1-9d58-6cc5a5191e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = load_data()\n",
    "X_train, X_valid, X_test = res[0], res[1], res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff0c3b-6956-4e7b-8d35-9011061d0d55",
   "metadata": {},
   "source": [
    "As was stated in README, we want to check if the model architecture and learning rate that we obtained during search for optimal hyperparameters for **omega** wil also satisfactorily work for **Pc** and **Tc**. Therefore, in order to avoid confusion, we create two classes - MyModel_omega, that will be trianed on **Pc** data with optimal number of layers and learning rate for **omega** prediction and MyModel_found. For the latter model we will first find optimal parameters and only than train and assess.  \n",
    "As you can see, classes differ only in activation function values, the general architecture (three linear layers, two activation fucnctions, and two dropouts) is the same for both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e0fa64-0bde-43e6-9fbb-c5e2c6e0e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel_omega(nn.Module):\n",
    "    def __init__(self, l1 = 2204, l2 = 2204):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(2204, l1)\n",
    "        self.a1 = nn.CELU(alpha = 0.01)\n",
    "        self.dropout_1 = nn.Dropout(p = 0.3)\n",
    "        self.linear_2 = nn.Linear(l1, l2)\n",
    "        self.a2 = nn.CELU(alpha = 0.01)\n",
    "        self.dropout_2 = nn.Dropout(p = 0.3)\n",
    "        self.linear_3 = nn.Linear(l2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.linear_3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "887dfa88-5184-4a14-8ea7-b4d86b7dfad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel_found(nn.Module):\n",
    "    def __init__(self, l1 = 2204, l2 = 2204):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(2204, l1)\n",
    "        self.a1 = nn.GELU(approximate='none')\n",
    "        self.dropout_1 = nn.Dropout(p = 0.3)\n",
    "        self.linear_2 = nn.Linear(l1, l2)\n",
    "        self.a2 = nn.GELU(approximate='none')\n",
    "        self.dropout_2 = nn.Dropout(p = 0.3)\n",
    "        self.linear_3 = nn.Linear(l2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.linear_3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467aa148-09dd-42a2-9ead-29cfc1099a45",
   "metadata": {},
   "source": [
    "This function is crucial for the search of optimal hyperparameters. It has following arguments:\n",
    "1. Config - dictionary with learning rate ('lr') and number of neurons in first and second linear layers ('l1' and 'l2')\n",
    "2. abs_path - is the path of *table_with_descriptors.csv* - second cell of this Notebook\n",
    "3.  Is_tune - flags if this function is used in Ray Tune instance to find optimal hyperparameters (True) or in model training with known hyperparameters (False)\n",
    "4.  model - if we use omega_model or found_model (described above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64209db5-8a73-4776-82ee-620c438a1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, abs_path = abs_path, is_tune = True, model = 'found'):\n",
    "    torch.manual_seed(1)\n",
    "    #model selection - found or omega. making the instance of the selected class\n",
    "    if model == 'found':\n",
    "        model = MyModel_found(config['l1'], config['l2'])\n",
    "    else:\n",
    "        model = MyModel_omega(config['l1'], config['l2'])\n",
    "    model.to('cuda:0')\n",
    "    loss_fn = MSELoss()\n",
    "    #optimizer with 'lr' from config\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config['lr'], weight_decay = 1e-4)\n",
    "    #loading data\n",
    "    data = load_data()\n",
    "    trainset, validset, testset = data[0], data[1], data[2]\n",
    "    column_transformer = data[3]\n",
    "    #is is_tune is selected, we create two DataLoaders - training and validation\n",
    "    if is_tune:\n",
    "        X_train_dl = DataLoader(trainset, shuffle = True, batch_size = 24)\n",
    "        X_valid_dl = DataLoader(validset, shuffle = True, batch_size = 24)\n",
    "   #if is_tune is not selected, so that we want to get our final model, we concatenate training and validation sets to train model on it\n",
    "    else:\n",
    "        X_train_dl = DataLoader(torch.utils.data.ConcatDataset([trainset, validset]), shuffle = True, batch_size = 24)\n",
    "    #here we train selected model for 200 epochs\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_b, y_b in X_train_dl:\n",
    "            X_b = X_b.to('cuda:0')\n",
    "            y_b = y_b.to('cuda:0')\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_b)\n",
    "            loss = loss_fn(pred.squeeze(), y_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        r2_valid = 0\n",
    "        rmse_valid = 0\n",
    "        valid_steps = 0\n",
    "        #if is_tune is not selected, we do not perform validation step - we will just check on the test set in the end\n",
    "        if is_tune:\n",
    "            for X_b, y_b in X_valid_dl:\n",
    "                X_b = X_b.to('cuda:0')\n",
    "                y_b = y_b.to('cuda:0')\n",
    "                pred = model(X_b)\n",
    "                loss = loss_fn(pred.squeeze(), y_b)\n",
    "                valid_loss += loss.detach().cpu().numpy()\n",
    "                r2_valid += r2_score(pred.squeeze().detach().cpu().numpy(), y_b.cpu().numpy())\n",
    "                rmse_valid += root_mean_squared_error(pred.squeeze().detach().cpu().numpy(), y_b.cpu().numpy())\n",
    "                valid_steps += 1\n",
    "        #report if is_tune is selected    \n",
    "        if is_tune:\n",
    "            train.report(\n",
    "                {\"loss\": valid_loss / valid_steps, \"r2\":r2_valid / valid_steps, \"rmse\":rmse_valid / valid_steps})\n",
    "    print('Finished training')\n",
    "    #if we just want to train the model, we return the model and column transformer\n",
    "    if not is_tune:\n",
    "        return model, column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62564f8a-bb2f-4b31-8d8c-6dbcb53855ac",
   "metadata": {},
   "source": [
    "We instantiate maximum number of epochs (it's constant so it's 200), make a scheduler (ASHAS Scheduler), select search space (config), select algorithm (HyperOptSearch) and then run the Ray Tune instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a515406f-56ee-4048-8da0-c1d412ef5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_epochs = 200\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=5,\n",
    "        reduction_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c5df571-d34c-4a70-95da-2f30b81b4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"l1\": tune.qrandint(100, 3000, 100),\n",
    "    \"l2\": tune.qrandint(100, 3000, 100),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "596c6a39-b3a1-467d-ae2c-a31ecac822d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt = HyperOptSearch(metric = 'loss', mode = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27931757-298a-4986-99a5-56e90886da8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 22:31:44,898\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-01-16 22:48:37</td></tr>\n",
       "<tr><td>Running for: </td><td>00:16:52.73        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.4/31.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=50<br>Bracket: Iter 160.000: -4.898591749595873 | Iter 80.000: -5.299614573969985 | Iter 40.000: -5.7738047729838975 | Iter 20.000: -6.440852450601982 | Iter 10.000: -7.272097760980779 | Iter 5.000: -8.61106236775716<br>Logical resource usage: 0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  l1</th><th style=\"text-align: right;\">  l2</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">      r2</th><th style=\"text-align: right;\">   rmse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_859bc131</td><td>TERMINATED</td><td>127.0.0.1:10176</td><td style=\"text-align: right;\">1300</td><td style=\"text-align: right;\">1400</td><td style=\"text-align: right;\">0.000820458</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">       100.095  </td><td style=\"text-align: right;\"> 4.96317</td><td style=\"text-align: right;\">0.94673 </td><td style=\"text-align: right;\">2.14132</td></tr>\n",
       "<tr><td>train_model_96825c68</td><td>TERMINATED</td><td>127.0.0.1:8048 </td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">1800</td><td style=\"text-align: right;\">0.00417381 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.34405</td><td style=\"text-align: right;\">44.1821 </td><td style=\"text-align: right;\">0.624913</td><td style=\"text-align: right;\">6.58393</td></tr>\n",
       "<tr><td>train_model_96416229</td><td>TERMINATED</td><td>127.0.0.1:13052</td><td style=\"text-align: right;\"> 600</td><td style=\"text-align: right;\">1700</td><td style=\"text-align: right;\">0.000307284</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         6.13799</td><td style=\"text-align: right;\"> 8.02213</td><td style=\"text-align: right;\">0.902821</td><td style=\"text-align: right;\">2.78032</td></tr>\n",
       "<tr><td>train_model_86249428</td><td>TERMINATED</td><td>127.0.0.1:5508 </td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">2100</td><td style=\"text-align: right;\">0.00444789 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.48449</td><td style=\"text-align: right;\">33.0663 </td><td style=\"text-align: right;\">0.504176</td><td style=\"text-align: right;\">5.68505</td></tr>\n",
       "<tr><td>train_model_001cfd98</td><td>TERMINATED</td><td>127.0.0.1:4020 </td><td style=\"text-align: right;\">1400</td><td style=\"text-align: right;\"> 900</td><td style=\"text-align: right;\">0.00385369 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         6.91231</td><td style=\"text-align: right;\"> 9.88565</td><td style=\"text-align: right;\">0.886498</td><td style=\"text-align: right;\">3.06597</td></tr>\n",
       "<tr><td>train_model_8d502dfd</td><td>TERMINATED</td><td>127.0.0.1:18584</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\"> 500</td><td style=\"text-align: right;\">0.000499715</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         8.80451</td><td style=\"text-align: right;\"> 6.81312</td><td style=\"text-align: right;\">0.925606</td><td style=\"text-align: right;\">2.56082</td></tr>\n",
       "<tr><td>train_model_8a38adc0</td><td>TERMINATED</td><td>127.0.0.1:12896</td><td style=\"text-align: right;\">1200</td><td style=\"text-align: right;\">1600</td><td style=\"text-align: right;\">0.00946834 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.54131</td><td style=\"text-align: right;\">14.8706 </td><td style=\"text-align: right;\">0.837025</td><td style=\"text-align: right;\">3.78094</td></tr>\n",
       "<tr><td>train_model_01709a44</td><td>TERMINATED</td><td>127.0.0.1:10200</td><td style=\"text-align: right;\">1300</td><td style=\"text-align: right;\">2100</td><td style=\"text-align: right;\">0.0011059  </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        12.5269 </td><td style=\"text-align: right;\"> 6.50932</td><td style=\"text-align: right;\">0.916662</td><td style=\"text-align: right;\">2.49341</td></tr>\n",
       "<tr><td>train_model_ace9d5e8</td><td>TERMINATED</td><td>127.0.0.1:1920 </td><td style=\"text-align: right;\">2200</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.000212338</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         9.01103</td><td style=\"text-align: right;\"> 8.67865</td><td style=\"text-align: right;\">0.899824</td><td style=\"text-align: right;\">2.89893</td></tr>\n",
       "<tr><td>train_model_15cba6f3</td><td>TERMINATED</td><td>127.0.0.1:648  </td><td style=\"text-align: right;\"> 600</td><td style=\"text-align: right;\">2700</td><td style=\"text-align: right;\">0.00030171 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.15488</td><td style=\"text-align: right;\"> 9.51347</td><td style=\"text-align: right;\">0.87604 </td><td style=\"text-align: right;\">3.01793</td></tr>\n",
       "<tr><td>train_model_bfe4c037</td><td>TERMINATED</td><td>127.0.0.1:16536</td><td style=\"text-align: right;\">2800</td><td style=\"text-align: right;\">2900</td><td style=\"text-align: right;\">0.000724168</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        39.2105 </td><td style=\"text-align: right;\"> 6.37197</td><td style=\"text-align: right;\">0.918394</td><td style=\"text-align: right;\">2.44062</td></tr>\n",
       "<tr><td>train_model_0dde6b02</td><td>TERMINATED</td><td>127.0.0.1:8260 </td><td style=\"text-align: right;\">2700</td><td style=\"text-align: right;\">2200</td><td style=\"text-align: right;\">0.000854648</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        18.577  </td><td style=\"text-align: right;\"> 8.10628</td><td style=\"text-align: right;\">0.894061</td><td style=\"text-align: right;\">2.76504</td></tr>\n",
       "<tr><td>train_model_296589db</td><td>TERMINATED</td><td>127.0.0.1:13068</td><td style=\"text-align: right;\">1600</td><td style=\"text-align: right;\"> 900</td><td style=\"text-align: right;\">0.00106569 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        12.1194 </td><td style=\"text-align: right;\"> 7.20792</td><td style=\"text-align: right;\">0.900408</td><td style=\"text-align: right;\">2.59163</td></tr>\n",
       "<tr><td>train_model_cf988ce0</td><td>TERMINATED</td><td>127.0.0.1:13592</td><td style=\"text-align: right;\">2500</td><td style=\"text-align: right;\">2300</td><td style=\"text-align: right;\">0.000969537</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        17.9566 </td><td style=\"text-align: right;\"> 7.34904</td><td style=\"text-align: right;\">0.918437</td><td style=\"text-align: right;\">2.64179</td></tr>\n",
       "<tr><td>train_model_5f42cd94</td><td>TERMINATED</td><td>127.0.0.1:7424 </td><td style=\"text-align: right;\"> 500</td><td style=\"text-align: right;\">1300</td><td style=\"text-align: right;\">0.00532209 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.91244</td><td style=\"text-align: right;\">10.8247 </td><td style=\"text-align: right;\">0.849236</td><td style=\"text-align: right;\">3.23759</td></tr>\n",
       "<tr><td>train_model_4d87e3b3</td><td>TERMINATED</td><td>127.0.0.1:3400 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">1900</td><td style=\"text-align: right;\">0.00035181 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         8.54187</td><td style=\"text-align: right;\"> 8.12834</td><td style=\"text-align: right;\">0.904833</td><td style=\"text-align: right;\">2.80059</td></tr>\n",
       "<tr><td>train_model_f24070d0</td><td>TERMINATED</td><td>127.0.0.1:2964 </td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">2400</td><td style=\"text-align: right;\">0.000407644</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.86103</td><td style=\"text-align: right;\"> 8.5934 </td><td style=\"text-align: right;\">0.894001</td><td style=\"text-align: right;\">2.85152</td></tr>\n",
       "<tr><td>train_model_1013f4c0</td><td>TERMINATED</td><td>127.0.0.1:13484</td><td style=\"text-align: right;\">1700</td><td style=\"text-align: right;\">2800</td><td style=\"text-align: right;\">0.000303766</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         5.46911</td><td style=\"text-align: right;\"> 8.68987</td><td style=\"text-align: right;\">0.904927</td><td style=\"text-align: right;\">2.89502</td></tr>\n",
       "<tr><td>train_model_fe5b4a3a</td><td>TERMINATED</td><td>127.0.0.1:8736 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">0.00895739 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.58806</td><td style=\"text-align: right;\">12.0557 </td><td style=\"text-align: right;\">0.860435</td><td style=\"text-align: right;\">3.4056 </td></tr>\n",
       "<tr><td>train_model_514aeb7d</td><td>TERMINATED</td><td>127.0.0.1:12456</td><td style=\"text-align: right;\">2200</td><td style=\"text-align: right;\"> 800</td><td style=\"text-align: right;\">0.000316151</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.90504</td><td style=\"text-align: right;\"> 9.69156</td><td style=\"text-align: right;\">0.882166</td><td style=\"text-align: right;\">3.06696</td></tr>\n",
       "<tr><td>train_model_98aa7d67</td><td>TERMINATED</td><td>127.0.0.1:14252</td><td style=\"text-align: right;\">2900</td><td style=\"text-align: right;\">1300</td><td style=\"text-align: right;\">0.000113423</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         5.73922</td><td style=\"text-align: right;\">10.5834 </td><td style=\"text-align: right;\">0.868553</td><td style=\"text-align: right;\">3.18544</td></tr>\n",
       "<tr><td>train_model_082ce977</td><td>TERMINATED</td><td>127.0.0.1:8520 </td><td style=\"text-align: right;\">1800</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">0.001825   </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         9.22227</td><td style=\"text-align: right;\">10.2601 </td><td style=\"text-align: right;\">0.879397</td><td style=\"text-align: right;\">3.15088</td></tr>\n",
       "<tr><td>train_model_62a14189</td><td>TERMINATED</td><td>127.0.0.1:5412 </td><td style=\"text-align: right;\"> 900</td><td style=\"text-align: right;\">1300</td><td style=\"text-align: right;\">0.00208836 </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        19.1459 </td><td style=\"text-align: right;\"> 7.12217</td><td style=\"text-align: right;\">0.920671</td><td style=\"text-align: right;\">2.57101</td></tr>\n",
       "<tr><td>train_model_b9e703f1</td><td>TERMINATED</td><td>127.0.0.1:9616 </td><td style=\"text-align: right;\">2900</td><td style=\"text-align: right;\"> 500</td><td style=\"text-align: right;\">0.000640693</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         5.28858</td><td style=\"text-align: right;\"> 9.93275</td><td style=\"text-align: right;\">0.867624</td><td style=\"text-align: right;\">3.08795</td></tr>\n",
       "<tr><td>train_model_0fb26762</td><td>TERMINATED</td><td>127.0.0.1:8256 </td><td style=\"text-align: right;\">2400</td><td style=\"text-align: right;\">2500</td><td style=\"text-align: right;\">0.00178677 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         6.08606</td><td style=\"text-align: right;\"> 8.71475</td><td style=\"text-align: right;\">0.900549</td><td style=\"text-align: right;\">2.90764</td></tr>\n",
       "<tr><td>train_model_424ce959</td><td>TERMINATED</td><td>127.0.0.1:17492</td><td style=\"text-align: right;\">1500</td><td style=\"text-align: right;\">1400</td><td style=\"text-align: right;\">0.00016074 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.62068</td><td style=\"text-align: right;\"> 9.53327</td><td style=\"text-align: right;\">0.87954 </td><td style=\"text-align: right;\">3.03005</td></tr>\n",
       "<tr><td>train_model_ca6ddbd5</td><td>TERMINATED</td><td>127.0.0.1:15368</td><td style=\"text-align: right;\"> 800</td><td style=\"text-align: right;\">1100</td><td style=\"text-align: right;\">0.000665681</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        83.4086 </td><td style=\"text-align: right;\"> 4.7067 </td><td style=\"text-align: right;\">0.945416</td><td style=\"text-align: right;\">2.07687</td></tr>\n",
       "<tr><td>train_model_78add24f</td><td>TERMINATED</td><td>127.0.0.1:2856 </td><td style=\"text-align: right;\">1900</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">0.0014334  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         9.44748</td><td style=\"text-align: right;\">11.5826 </td><td style=\"text-align: right;\">0.851444</td><td style=\"text-align: right;\">3.31276</td></tr>\n",
       "<tr><td>train_model_5fa54858</td><td>TERMINATED</td><td>127.0.0.1:17988</td><td style=\"text-align: right;\"> 900</td><td style=\"text-align: right;\">1100</td><td style=\"text-align: right;\">0.00278235 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.17121</td><td style=\"text-align: right;\">12.69   </td><td style=\"text-align: right;\">0.861573</td><td style=\"text-align: right;\">3.4872 </td></tr>\n",
       "<tr><td>train_model_50a08222</td><td>TERMINATED</td><td>127.0.0.1:16472</td><td style=\"text-align: right;\">1100</td><td style=\"text-align: right;\"> 500</td><td style=\"text-align: right;\">0.000645244</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        35.5372 </td><td style=\"text-align: right;\"> 5.45547</td><td style=\"text-align: right;\">0.935808</td><td style=\"text-align: right;\">2.25394</td></tr>\n",
       "<tr><td>train_model_db8952a5</td><td>TERMINATED</td><td>127.0.0.1:16704</td><td style=\"text-align: right;\"> 700</td><td style=\"text-align: right;\">1800</td><td style=\"text-align: right;\">0.000547585</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        35.3874 </td><td style=\"text-align: right;\"> 5.46057</td><td style=\"text-align: right;\">0.94237 </td><td style=\"text-align: right;\">2.27536</td></tr>\n",
       "<tr><td>train_model_e163cd3a</td><td>TERMINATED</td><td>127.0.0.1:4672 </td><td style=\"text-align: right;\"> 400</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">0.000205607</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         3.86809</td><td style=\"text-align: right;\">14.0898 </td><td style=\"text-align: right;\">0.814307</td><td style=\"text-align: right;\">3.68897</td></tr>\n",
       "<tr><td>train_model_04f69114</td><td>TERMINATED</td><td>127.0.0.1:10512</td><td style=\"text-align: right;\"> 800</td><td style=\"text-align: right;\">1500</td><td style=\"text-align: right;\">0.00134646 </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">        10.8612 </td><td style=\"text-align: right;\"> 7.0707 </td><td style=\"text-align: right;\">0.92164 </td><td style=\"text-align: right;\">2.58187</td></tr>\n",
       "<tr><td>train_model_5b7e09ab</td><td>TERMINATED</td><td>127.0.0.1:19112</td><td style=\"text-align: right;\"> 400</td><td style=\"text-align: right;\">1100</td><td style=\"text-align: right;\">0.00292919 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.65277</td><td style=\"text-align: right;\"> 9.86534</td><td style=\"text-align: right;\">0.876485</td><td style=\"text-align: right;\">3.10036</td></tr>\n",
       "<tr><td>train_model_2be14cff</td><td>TERMINATED</td><td>127.0.0.1:8240 </td><td style=\"text-align: right;\">1100</td><td style=\"text-align: right;\">1700</td><td style=\"text-align: right;\">0.0061688  </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.41356</td><td style=\"text-align: right;\">21.2725 </td><td style=\"text-align: right;\">0.654342</td><td style=\"text-align: right;\">4.57521</td></tr>\n",
       "<tr><td>train_model_f40457bd</td><td>TERMINATED</td><td>127.0.0.1:13344</td><td style=\"text-align: right;\">1400</td><td style=\"text-align: right;\"> 700</td><td style=\"text-align: right;\">0.000102908</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.39721</td><td style=\"text-align: right;\">13.0493 </td><td style=\"text-align: right;\">0.830974</td><td style=\"text-align: right;\">3.56047</td></tr>\n",
       "<tr><td>train_model_21054be6</td><td>TERMINATED</td><td>127.0.0.1:8088 </td><td style=\"text-align: right;\">1300</td><td style=\"text-align: right;\">1100</td><td style=\"text-align: right;\">0.000236731</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.6291 </td><td style=\"text-align: right;\"> 8.84043</td><td style=\"text-align: right;\">0.888281</td><td style=\"text-align: right;\">2.91846</td></tr>\n",
       "<tr><td>train_model_20f13cb6</td><td>TERMINATED</td><td>127.0.0.1:19340</td><td style=\"text-align: right;\"> 300</td><td style=\"text-align: right;\"> 300</td><td style=\"text-align: right;\">0.000474444</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5.4615 </td><td style=\"text-align: right;\"> 7.70605</td><td style=\"text-align: right;\">0.907994</td><td style=\"text-align: right;\">2.71949</td></tr>\n",
       "<tr><td>train_model_48c205f1</td><td>TERMINATED</td><td>127.0.0.1:19672</td><td style=\"text-align: right;\"> 700</td><td style=\"text-align: right;\">1600</td><td style=\"text-align: right;\">0.00015842 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.29903</td><td style=\"text-align: right;\">10.3423 </td><td style=\"text-align: right;\">0.86503 </td><td style=\"text-align: right;\">3.16194</td></tr>\n",
       "<tr><td>train_model_6002ea0a</td><td>TERMINATED</td><td>127.0.0.1:3004 </td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\"> 700</td><td style=\"text-align: right;\">0.00133961 </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">        16.7867 </td><td style=\"text-align: right;\"> 6.03488</td><td style=\"text-align: right;\">0.931718</td><td style=\"text-align: right;\">2.38518</td></tr>\n",
       "<tr><td>train_model_2fb16823</td><td>TERMINATED</td><td>127.0.0.1:1536 </td><td style=\"text-align: right;\">1200</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">0.000814341</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">        38.9385 </td><td style=\"text-align: right;\"> 6.48648</td><td style=\"text-align: right;\">0.927153</td><td style=\"text-align: right;\">2.47535</td></tr>\n",
       "<tr><td>train_model_de72bc97</td><td>TERMINATED</td><td>127.0.0.1:15480</td><td style=\"text-align: right;\">1600</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">0.00274608 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.97237</td><td style=\"text-align: right;\">12.453  </td><td style=\"text-align: right;\">0.886255</td><td style=\"text-align: right;\">3.47392</td></tr>\n",
       "<tr><td>train_model_9c0e3d3e</td><td>TERMINATED</td><td>127.0.0.1:16928</td><td style=\"text-align: right;\"> 900</td><td style=\"text-align: right;\">1500</td><td style=\"text-align: right;\">0.000429035</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         6.96264</td><td style=\"text-align: right;\"> 7.64077</td><td style=\"text-align: right;\">0.91798 </td><td style=\"text-align: right;\">2.70307</td></tr>\n",
       "<tr><td>train_model_14e61e36</td><td>TERMINATED</td><td>127.0.0.1:15956</td><td style=\"text-align: right;\">1400</td><td style=\"text-align: right;\">1800</td><td style=\"text-align: right;\">0.00355253 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.90258</td><td style=\"text-align: right;\">22.0426 </td><td style=\"text-align: right;\">0.718496</td><td style=\"text-align: right;\">4.63988</td></tr>\n",
       "<tr><td>train_model_2196b738</td><td>TERMINATED</td><td>127.0.0.1:16412</td><td style=\"text-align: right;\">1100</td><td style=\"text-align: right;\"> 400</td><td style=\"text-align: right;\">0.00107386 </td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">        69.1603 </td><td style=\"text-align: right;\"> 5.21758</td><td style=\"text-align: right;\">0.945376</td><td style=\"text-align: right;\">2.20032</td></tr>\n",
       "<tr><td>train_model_6dbea4e4</td><td>TERMINATED</td><td>127.0.0.1:18224</td><td style=\"text-align: right;\"> 600</td><td style=\"text-align: right;\">1200</td><td style=\"text-align: right;\">0.000250465</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.02816</td><td style=\"text-align: right;\"> 9.46322</td><td style=\"text-align: right;\">0.886284</td><td style=\"text-align: right;\">3.04164</td></tr>\n",
       "<tr><td>train_model_9b2c33f6</td><td>TERMINATED</td><td>127.0.0.1:13492</td><td style=\"text-align: right;\">2200</td><td style=\"text-align: right;\"> 900</td><td style=\"text-align: right;\">0.000532073</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         4.76525</td><td style=\"text-align: right;\"> 8.88227</td><td style=\"text-align: right;\">0.89249 </td><td style=\"text-align: right;\">2.91491</td></tr>\n",
       "<tr><td>train_model_fe0c6a39</td><td>TERMINATED</td><td>127.0.0.1:4164 </td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 700</td><td style=\"text-align: right;\">0.000796986</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        84.6672 </td><td style=\"text-align: right;\"> 4.85057</td><td style=\"text-align: right;\">0.941992</td><td style=\"text-align: right;\">2.11832</td></tr>\n",
       "<tr><td>train_model_1d727fc1</td><td>TERMINATED</td><td>127.0.0.1:18360</td><td style=\"text-align: right;\">1700</td><td style=\"text-align: right;\">2600</td><td style=\"text-align: right;\">0.000372602</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         8.91421</td><td style=\"text-align: right;\"> 8.00523</td><td style=\"text-align: right;\">0.913057</td><td style=\"text-align: right;\">2.77603</td></tr>\n",
       "<tr><td>train_model_ae515cc6</td><td>TERMINATED</td><td>127.0.0.1:4356 </td><td style=\"text-align: right;\"> 500</td><td style=\"text-align: right;\"> 600</td><td style=\"text-align: right;\">0.000845436</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         9.80824</td><td style=\"text-align: right;\"> 6.45284</td><td style=\"text-align: right;\">0.92388 </td><td style=\"text-align: right;\">2.45632</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">      r2</th><th style=\"text-align: right;\">   rmse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_001cfd98</td><td style=\"text-align: right;\"> 9.88565</td><td style=\"text-align: right;\">0.886498</td><td style=\"text-align: right;\">3.06597</td></tr>\n",
       "<tr><td>train_model_01709a44</td><td style=\"text-align: right;\"> 6.50932</td><td style=\"text-align: right;\">0.916662</td><td style=\"text-align: right;\">2.49341</td></tr>\n",
       "<tr><td>train_model_04f69114</td><td style=\"text-align: right;\"> 7.0707 </td><td style=\"text-align: right;\">0.92164 </td><td style=\"text-align: right;\">2.58187</td></tr>\n",
       "<tr><td>train_model_082ce977</td><td style=\"text-align: right;\">10.2601 </td><td style=\"text-align: right;\">0.879397</td><td style=\"text-align: right;\">3.15088</td></tr>\n",
       "<tr><td>train_model_0dde6b02</td><td style=\"text-align: right;\"> 8.10628</td><td style=\"text-align: right;\">0.894061</td><td style=\"text-align: right;\">2.76504</td></tr>\n",
       "<tr><td>train_model_0fb26762</td><td style=\"text-align: right;\"> 8.71475</td><td style=\"text-align: right;\">0.900549</td><td style=\"text-align: right;\">2.90764</td></tr>\n",
       "<tr><td>train_model_1013f4c0</td><td style=\"text-align: right;\"> 8.68987</td><td style=\"text-align: right;\">0.904927</td><td style=\"text-align: right;\">2.89502</td></tr>\n",
       "<tr><td>train_model_14e61e36</td><td style=\"text-align: right;\">22.0426 </td><td style=\"text-align: right;\">0.718496</td><td style=\"text-align: right;\">4.63988</td></tr>\n",
       "<tr><td>train_model_15cba6f3</td><td style=\"text-align: right;\"> 9.51347</td><td style=\"text-align: right;\">0.87604 </td><td style=\"text-align: right;\">3.01793</td></tr>\n",
       "<tr><td>train_model_1d727fc1</td><td style=\"text-align: right;\"> 8.00523</td><td style=\"text-align: right;\">0.913057</td><td style=\"text-align: right;\">2.77603</td></tr>\n",
       "<tr><td>train_model_20f13cb6</td><td style=\"text-align: right;\"> 7.70605</td><td style=\"text-align: right;\">0.907994</td><td style=\"text-align: right;\">2.71949</td></tr>\n",
       "<tr><td>train_model_21054be6</td><td style=\"text-align: right;\"> 8.84043</td><td style=\"text-align: right;\">0.888281</td><td style=\"text-align: right;\">2.91846</td></tr>\n",
       "<tr><td>train_model_2196b738</td><td style=\"text-align: right;\"> 5.21758</td><td style=\"text-align: right;\">0.945376</td><td style=\"text-align: right;\">2.20032</td></tr>\n",
       "<tr><td>train_model_296589db</td><td style=\"text-align: right;\"> 7.20792</td><td style=\"text-align: right;\">0.900408</td><td style=\"text-align: right;\">2.59163</td></tr>\n",
       "<tr><td>train_model_2be14cff</td><td style=\"text-align: right;\">21.2725 </td><td style=\"text-align: right;\">0.654342</td><td style=\"text-align: right;\">4.57521</td></tr>\n",
       "<tr><td>train_model_2fb16823</td><td style=\"text-align: right;\"> 6.48648</td><td style=\"text-align: right;\">0.927153</td><td style=\"text-align: right;\">2.47535</td></tr>\n",
       "<tr><td>train_model_424ce959</td><td style=\"text-align: right;\"> 9.53327</td><td style=\"text-align: right;\">0.87954 </td><td style=\"text-align: right;\">3.03005</td></tr>\n",
       "<tr><td>train_model_48c205f1</td><td style=\"text-align: right;\">10.3423 </td><td style=\"text-align: right;\">0.86503 </td><td style=\"text-align: right;\">3.16194</td></tr>\n",
       "<tr><td>train_model_4d87e3b3</td><td style=\"text-align: right;\"> 8.12834</td><td style=\"text-align: right;\">0.904833</td><td style=\"text-align: right;\">2.80059</td></tr>\n",
       "<tr><td>train_model_50a08222</td><td style=\"text-align: right;\"> 5.45547</td><td style=\"text-align: right;\">0.935808</td><td style=\"text-align: right;\">2.25394</td></tr>\n",
       "<tr><td>train_model_514aeb7d</td><td style=\"text-align: right;\"> 9.69156</td><td style=\"text-align: right;\">0.882166</td><td style=\"text-align: right;\">3.06696</td></tr>\n",
       "<tr><td>train_model_5b7e09ab</td><td style=\"text-align: right;\"> 9.86534</td><td style=\"text-align: right;\">0.876485</td><td style=\"text-align: right;\">3.10036</td></tr>\n",
       "<tr><td>train_model_5f42cd94</td><td style=\"text-align: right;\">10.8247 </td><td style=\"text-align: right;\">0.849236</td><td style=\"text-align: right;\">3.23759</td></tr>\n",
       "<tr><td>train_model_5fa54858</td><td style=\"text-align: right;\">12.69   </td><td style=\"text-align: right;\">0.861573</td><td style=\"text-align: right;\">3.4872 </td></tr>\n",
       "<tr><td>train_model_6002ea0a</td><td style=\"text-align: right;\"> 6.03488</td><td style=\"text-align: right;\">0.931718</td><td style=\"text-align: right;\">2.38518</td></tr>\n",
       "<tr><td>train_model_62a14189</td><td style=\"text-align: right;\"> 7.12217</td><td style=\"text-align: right;\">0.920671</td><td style=\"text-align: right;\">2.57101</td></tr>\n",
       "<tr><td>train_model_6dbea4e4</td><td style=\"text-align: right;\"> 9.46322</td><td style=\"text-align: right;\">0.886284</td><td style=\"text-align: right;\">3.04164</td></tr>\n",
       "<tr><td>train_model_78add24f</td><td style=\"text-align: right;\">11.5826 </td><td style=\"text-align: right;\">0.851444</td><td style=\"text-align: right;\">3.31276</td></tr>\n",
       "<tr><td>train_model_859bc131</td><td style=\"text-align: right;\"> 4.96317</td><td style=\"text-align: right;\">0.94673 </td><td style=\"text-align: right;\">2.14132</td></tr>\n",
       "<tr><td>train_model_86249428</td><td style=\"text-align: right;\">33.0663 </td><td style=\"text-align: right;\">0.504176</td><td style=\"text-align: right;\">5.68505</td></tr>\n",
       "<tr><td>train_model_8a38adc0</td><td style=\"text-align: right;\">14.8706 </td><td style=\"text-align: right;\">0.837025</td><td style=\"text-align: right;\">3.78094</td></tr>\n",
       "<tr><td>train_model_8d502dfd</td><td style=\"text-align: right;\"> 6.81312</td><td style=\"text-align: right;\">0.925606</td><td style=\"text-align: right;\">2.56082</td></tr>\n",
       "<tr><td>train_model_96416229</td><td style=\"text-align: right;\"> 8.02213</td><td style=\"text-align: right;\">0.902821</td><td style=\"text-align: right;\">2.78032</td></tr>\n",
       "<tr><td>train_model_96825c68</td><td style=\"text-align: right;\">44.1821 </td><td style=\"text-align: right;\">0.624913</td><td style=\"text-align: right;\">6.58393</td></tr>\n",
       "<tr><td>train_model_98aa7d67</td><td style=\"text-align: right;\">10.5834 </td><td style=\"text-align: right;\">0.868553</td><td style=\"text-align: right;\">3.18544</td></tr>\n",
       "<tr><td>train_model_9b2c33f6</td><td style=\"text-align: right;\"> 8.88227</td><td style=\"text-align: right;\">0.89249 </td><td style=\"text-align: right;\">2.91491</td></tr>\n",
       "<tr><td>train_model_9c0e3d3e</td><td style=\"text-align: right;\"> 7.64077</td><td style=\"text-align: right;\">0.91798 </td><td style=\"text-align: right;\">2.70307</td></tr>\n",
       "<tr><td>train_model_ace9d5e8</td><td style=\"text-align: right;\"> 8.67865</td><td style=\"text-align: right;\">0.899824</td><td style=\"text-align: right;\">2.89893</td></tr>\n",
       "<tr><td>train_model_ae515cc6</td><td style=\"text-align: right;\"> 6.45284</td><td style=\"text-align: right;\">0.92388 </td><td style=\"text-align: right;\">2.45632</td></tr>\n",
       "<tr><td>train_model_b9e703f1</td><td style=\"text-align: right;\"> 9.93275</td><td style=\"text-align: right;\">0.867624</td><td style=\"text-align: right;\">3.08795</td></tr>\n",
       "<tr><td>train_model_bfe4c037</td><td style=\"text-align: right;\"> 6.37197</td><td style=\"text-align: right;\">0.918394</td><td style=\"text-align: right;\">2.44062</td></tr>\n",
       "<tr><td>train_model_ca6ddbd5</td><td style=\"text-align: right;\"> 4.7067 </td><td style=\"text-align: right;\">0.945416</td><td style=\"text-align: right;\">2.07687</td></tr>\n",
       "<tr><td>train_model_cf988ce0</td><td style=\"text-align: right;\"> 7.34904</td><td style=\"text-align: right;\">0.918437</td><td style=\"text-align: right;\">2.64179</td></tr>\n",
       "<tr><td>train_model_db8952a5</td><td style=\"text-align: right;\"> 5.46057</td><td style=\"text-align: right;\">0.94237 </td><td style=\"text-align: right;\">2.27536</td></tr>\n",
       "<tr><td>train_model_de72bc97</td><td style=\"text-align: right;\">12.453  </td><td style=\"text-align: right;\">0.886255</td><td style=\"text-align: right;\">3.47392</td></tr>\n",
       "<tr><td>train_model_e163cd3a</td><td style=\"text-align: right;\">14.0898 </td><td style=\"text-align: right;\">0.814307</td><td style=\"text-align: right;\">3.68897</td></tr>\n",
       "<tr><td>train_model_f24070d0</td><td style=\"text-align: right;\"> 8.5934 </td><td style=\"text-align: right;\">0.894001</td><td style=\"text-align: right;\">2.85152</td></tr>\n",
       "<tr><td>train_model_f40457bd</td><td style=\"text-align: right;\">13.0493 </td><td style=\"text-align: right;\">0.830974</td><td style=\"text-align: right;\">3.56047</td></tr>\n",
       "<tr><td>train_model_fe0c6a39</td><td style=\"text-align: right;\"> 4.85057</td><td style=\"text-align: right;\">0.941992</td><td style=\"text-align: right;\">2.11832</td></tr>\n",
       "<tr><td>train_model_fe5b4a3a</td><td style=\"text-align: right;\">12.0557 </td><td style=\"text-align: right;\">0.860435</td><td style=\"text-align: right;\">3.4056 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=10176)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=8048)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=13052)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=5508)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=4020)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=18584)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=12896)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=10200)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=1920)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=648)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=16536)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=8260)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=13068)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=13592)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=7424)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=3400)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=2964)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=13484)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=8736)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=12456)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=14252)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=8520)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=5412)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=9616)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=8256)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=17492)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=15368)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=2856)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=17988)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=16472)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=16704)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=4672)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=19112)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=8240)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=13344)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=19340)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=19672)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=3004)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=1536)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=15480)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=16928)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=15956)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=16412)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=18224)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=13492)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=4164)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "\u001b[36m(func pid=18360)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n",
      "2025-01-16 22:48:37,638\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/Timur/ray_results/train_model_2025-01-16_22-31-44' in 0.0422s.\n",
      "2025-01-16 22:48:37,647\tINFO tune.py:1041 -- Total run time: 1012.75 seconds (1012.68 seconds for the tuning loop).\n",
      "\u001b[36m(func pid=4356)\u001b[0m D:\\bld\\apache-arrow_1692865689659\\work\\cpp\\src\\arrow\\filesystem\\s3fs.cc:2829:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.  This could lead to a segmentation fault at exit\n"
     ]
    }
   ],
   "source": [
    "result = tune.run(partial(train_model), search_alg= hyperopt, config = config, num_samples = 50, scheduler = scheduler, resources_per_trial={\"gpu\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8aba5-8347-4533-aca5-54fb0e9d9080",
   "metadata": {},
   "source": [
    "Let's save optimal result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f2e8946-9076-4737-ab83-ed68d8ed78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('run_result_pressure.pickle', 'wb') as output:\n",
    "    pickle.dump(result, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3c19c-79c6-4df4-8b1d-5ba134c1ef34",
   "metadata": {},
   "source": [
    "And then we start to assess models. First, we load best configuration for **omega** prediction and train the model on the concatenated *train* and *valid* datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd74470b-d11a-405e-bf31-b595fe2db557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l1': 2700, 'l2': 900, 'lr': 0.00010525231047880925}\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "with open('run_result_omega.pickle', 'rb') as inp:\n",
    "    result1 = pickle.load(inp)\n",
    "\n",
    "best_config_1 = result1.get_best_config(metric = 'rmse', mode = 'min')\n",
    "print(best_config_1)\n",
    "model_pressure_1, column_transformer_1 = train_model(config = best_config_1, is_tune = False, model = 'own')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463dec1a-4f63-4435-ac4a-dac0e1cf18db",
   "metadata": {},
   "source": [
    "Then we assess the model on the *test* dataset, which we loaded on the 4th step of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c0d4f09-3310-4250-862c-29a78c1197e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score is 0.9548234939575195.\n",
      "RMSE score is 2.1996543407440186.\n"
     ]
    }
   ],
   "source": [
    "model_pressure_1.eval()\n",
    "pred = model_pressure_1(X_test.tensors[0].to('cuda:0')).squeeze().detach().cpu().numpy()\n",
    "print('R2 score is {}.'.format(r2_score(pred, X_test.tensors[1].cpu().numpy())))\n",
    "print('RMSE score is {}.'.format(root_mean_squared_error(pred, X_test.tensors[1].cpu().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afae4a0-4800-4139-b310-47565f17c5ad",
   "metadata": {},
   "source": [
    "Then we perform the same operations for the best configuration for **Pc** prediction, found in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b23e503-ed60-4478-acc9-116acf0b7b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l1': 800, 'l2': 1100, 'lr': 0.0006656808561573917}\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "with open('run_result_pressure.pickle', 'rb') as inp:\n",
    "    result2 = pickle.load(inp)\n",
    "\n",
    "best_config_2 = result2.get_best_config(metric = 'rmse', mode = 'min')\n",
    "print(best_config_2)\n",
    "model_pressure_2, column_transformer2 = train_model(config = best_config_2, is_tune = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58348b46-1b8e-48a8-ab59-b0476133b348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for second model is 0.9622953534126282.\n",
      "RMSE score for second model is 1.9567097425460815.\n"
     ]
    }
   ],
   "source": [
    "model_pressure_2.eval()\n",
    "pred = model_pressure_2(X_test.tensors[0].to('cuda:0')).squeeze().detach().cpu().numpy()\n",
    "print('R2 score for second model is {}.'.format(r2_score(pred, X_test.tensors[1].cpu().numpy())))\n",
    "print('RMSE score for second model is {}.'.format(root_mean_squared_error(pred, X_test.tensors[1].cpu().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d796f-6192-4976-a912-d1fa77388b70",
   "metadata": {},
   "source": [
    "As we can see, performance of both models is quite close. Therefore, to ease coding and reduce confusion, in the final calculator we will use model_1, which uses optimal configuration for **omega**. Model_2 is also saved. Column_transformer instances is the same for both models, so we save it only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a1f6cc-d0a3-43ec-a159-72605e608745",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('column_transformer_Pc.pickle', 'wb') as output:\n",
    "    pickle.dump(column_transformer_1, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c61d94a7-5a6b-4466-8b97-4f69cf2c5ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_pressure_1.state_dict(), 'model_1_dict_state.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c9bcddf-470b-4857-83d6-98b7c183a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_pressure_2.state_dict(), 'model_2_dict_state.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
